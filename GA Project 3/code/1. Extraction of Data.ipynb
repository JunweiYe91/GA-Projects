{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3004df5",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Web APIs & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ef605",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem Statement\n",
    "You are a data scientist in a well known cosmetic company. The company has an online store and discussion forums for all the product categories. The department in charge of the website and forum has complained that customers are posting their feedbacks and discussions for Makeup and Perfumes in the wrong forums. Due to this, you colleagues have to manually comb through the discussion forums to identify posts that have been posted in the wrong forum. They found this time consuming and have turned to the data science department to look for a better solution.\n",
    "\n",
    "You have been tasked by your direct supervisor to create a classification model to identify whether posts should be classified in the Makeup or Perfumes column. At the same time, use the analysis to identify useful insights that could be used by the marketing department for their marketing efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373e4d6",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Datasets Used](#Datasets-Used)\n",
    "- [Extraction of Data](#Extraction-of-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3bd23",
   "metadata": {},
   "source": [
    "## Datasets Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c4d1d",
   "metadata": {},
   "source": [
    "As the forums were newly created, there isn't sufficient data for us to perform analysis on. As such, we have extract posts from the Makeup and Perfume subreddits to form our dataset.\n",
    "\n",
    "Information found in the datasets include the title and selftext of the posts. Please refer to the data dictionary for more infomation on the columns extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89f6d8",
   "metadata": {},
   "source": [
    "## Extraction of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc35d7",
   "metadata": {},
   "source": [
    "**Install `psaw` library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fcc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install psaw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529bc34",
   "metadata": {},
   "source": [
    "Use the above to install the `psaw` library if it is not available in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b69948",
   "metadata": {},
   "source": [
    "**1. Importing of libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a83a1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt \n",
    "import time\n",
    "import random\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3ac28",
   "metadata": {},
   "source": [
    "**2. Extraction of data using Pushshift.io and putting data into dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19948d6e",
   "metadata": {},
   "source": [
    "Our goal is to get about 3,000 posts from each of the subreddit. We will create a function and extract posts from both subreddits. We will start from the most recent posts and work our way back till we get at least 3,000 posts from each subreddit. Data is extracted on 27 August 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b87a3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(subreddit, n, days = 30):\n",
    "    \n",
    "    # Url\n",
    "    base_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    full_url = f'{base_url}?subreddit={subreddit}&size=500'\n",
    "    \n",
    "    # Creating an empty list to store the posts\n",
    "    posts = []\n",
    "    \n",
    "    # Iterations to modify the url after each iteration\n",
    "    for i in range(1, n+1):\n",
    "        urlmod = '{}&after={}d'.format(full_url, days*i)\n",
    "        res_1 = requests.get(urlmod)\n",
    "        \n",
    "        # This is to prevent errors from stopping the codes from running\n",
    "        try:\n",
    "            res = requests.get(urlmod)\n",
    "            assert res.status_code == 200\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Converting to json\n",
    "        extracted = res.json()['data']\n",
    "        # Constructing a dataframe from dict\n",
    "        df = pd.DataFrame.from_dict(extracted)\n",
    "        # Adding the df to post list(created on top)\n",
    "        posts.append(df)\n",
    "        \n",
    "        # Total number of posts scrapped\n",
    "        total_scraped = sum(len(x) for x in posts)\n",
    "        \n",
    "        # If there are more than n values/data, stop. \n",
    "        if total_scraped > n:\n",
    "            break\n",
    "        \n",
    "        # Generate a random sleep duration to seem like a human user\n",
    "        sleep_duration = random.randint(1,9)\n",
    "        time.sleep(sleep_duration)\n",
    "            \n",
    "    \n",
    "    # Creating a list of features of interest that we will be using\n",
    "    features_of_interest = ['subreddit', 'title', 'selftext']\n",
    "    \n",
    "    # Combine all iterations into 1 dataframe\n",
    "    final_df = pd.concat(posts, sort=False)\n",
    "    # And remove all the unrequired columns from the datasets\n",
    "    final_df = final_df[features_of_interest]\n",
    "    # Dropping any duplicates\n",
    "    final_df.drop_duplicates(inplace=True)\n",
    "    return final_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fbf7ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3002 submissions on 'Perfumes' from Pushshift\n",
      "Retrieved 3220 submissions on 'Makeup' from Pushshift\n"
     ]
    }
   ],
   "source": [
    "submissions_perfumes_df = scrap('Perfumes', 3000)\n",
    "submissions_makeup_df = scrap('Makeup', 3000)\n",
    "\n",
    "print(f'Retrieved {len(submissions_perfumes_df)} submissions on \\'Perfumes\\' from Pushshift')\n",
    "print((f'Retrieved {len(submissions_makeup_df)} submissions on \\'Makeup\\' from Pushshift'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d84f64b",
   "metadata": {},
   "source": [
    "We have managed to extract more than 3,000 non duplicate posts for both subreddits. We will export the files to csv and proceed with the cleaning and analysis of the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7194b69",
   "metadata": {},
   "source": [
    "**5. Exporting of data to csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100799f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_perfumes_df.to_csv('../datasets/perfumes_df.csv')\n",
    "submissions_makeup_df.to_csv('../datasets/makeup_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69edf3d2",
   "metadata": {},
   "source": [
    "We will continue the rest of the analysis in a separate workbook. Please refer to **\"2. Analysis of Datasets\"** for the analysis and recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
