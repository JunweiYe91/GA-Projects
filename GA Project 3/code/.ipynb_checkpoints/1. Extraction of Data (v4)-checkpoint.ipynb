{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3004df5",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Project 3: Web APIs & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ef605",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem Statement\n",
    "You are a data scientist in a well known real estate company located in Ames. In a bid to boost sales, the Board of Directors wants to provide free self-served platform to inform clients of the potential value of their homes. They would also like to find identify factors that might affect sale prices as higher sale prices equate to higher commission income. \n",
    "\n",
    "You have been tasked by your direct supervisor to create a regression model to predict the price of houses in Ames, so that these prices can be included in the platform. You will also need to identify factors affecting sales price and make recommendations on what could be done to improve sales income."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373e4d6",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Background](#Background)\n",
    "- [Datasets Used](#Datasets-Used)\n",
    "- [Extraction of Data](#Extraction-of-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf8c04",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa68883",
   "metadata": {},
   "source": [
    "Ames is a city in Story County, Iowa, United States, located approximately 30 miles (48 km) north of Des Moines in central Iowa. ([*source*](https://en.wikipedia.org/wiki/Ames,_Iowa)). With a population of more than 65,000, Ames offers cultural, recreational, educational, business, and entertainment amenities more common in bigger metros. As a growing city, Ames continues to focus on building a strong community filled with opportunities for all. ([*source*](https://www.cityofames.org/about-ames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3bd23",
   "metadata": {},
   "source": [
    "## Datasets Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c4d1d",
   "metadata": {},
   "source": [
    "For the purpose of the analysis, we are provided with the `train` and `test` datasets. The `train` dataset contains Ames' housing sales prices and their relevant information from 2006 to 2010. We will be using this dataset for model building purposes. The `test` dataset contains another set of Ames' housing sale price, but does not include the sale prices. We predicting the sale prices found in this dataset instead.\n",
    "\n",
    "Information found in the `train` datasets includes information suchs as the sale prices, building class, information on the pool, basement, neighbourhood, garage and overall quality of the house. The full information could be found in the data dictionary below.\n",
    "\n",
    "Information found in the `test` datasets contains the same fields as those found in thte `train` dataset, except for the sale prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89f6d8",
   "metadata": {},
   "source": [
    "## Extraction of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc35d7",
   "metadata": {},
   "source": [
    "**Install `psaw` library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fcc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install psaw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529bc34",
   "metadata": {},
   "source": [
    "Use the above to install the `psaw` library if it is not available in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b69948",
   "metadata": {},
   "source": [
    "**1. Importing of libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a83a1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt \n",
    "import time\n",
    "import random\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ee5cd",
   "metadata": {},
   "source": [
    "**2. Define the date range for the extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c4efd0",
   "metadata": {},
   "source": [
    "The before arguments in pmaw only accept dates in the epoch time format, which is the number of seconds that have elapsed since 00:00:00 UTC on Jan 1, 1970. Thus we will use the below function to convert 15$^{th}$ August 2022 to epoch time format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0738e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = int(dt.datetime(2022,8,15,0,0).timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3ac28",
   "metadata": {},
   "source": [
    "**3. Extraction of data using Pushshift.io and putting data into dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19948d6e",
   "metadata": {},
   "source": [
    "Our goal is to get about 3,000 posts from each of the subreddit. We will create a function and extract posts from both subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87a3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(subreddit, n, days = 30):\n",
    "    \n",
    "    # Url\n",
    "    base_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    full_url = f'{base_url}?subreddit={subreddit}&size=500'\n",
    "    #print(full_url)\n",
    "    \n",
    "    # Creating an empty list to store the posts\n",
    "    posts = []\n",
    "    \n",
    "    # Iterations to modify the url after each iteration\n",
    "    for i in range(1, n+1):\n",
    "        urlmod = '{}&after={}d'.format(full_url, days*i)\n",
    "        #print URL used and days\n",
    "        #print(f'Url: {urlmod}')\n",
    "        #print(f'Days: {days*i}')\n",
    "        res_1 = requests.get(urlmod)\n",
    "        \n",
    "        # This is to prevent errors from stopping the codes from running\n",
    "        try:\n",
    "            res = requests.get(urlmod)\n",
    "            assert res.status_code == 200\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Converting to json\n",
    "        extracted = res.json()['data']\n",
    "        # Constructing a dataframe from dict\n",
    "        df = pd.DataFrame.from_dict(extracted)\n",
    "        # Adding the df to post list(created on top)\n",
    "        posts.append(df)\n",
    "        \n",
    "        \n",
    "        total_scraped = sum(len(x) for x in posts)\n",
    "        # Print total posts scrapped to see how many posts the function has scrapped\n",
    "        #print(f'total_scraped: {total_scraped}')\n",
    "        \n",
    "        \n",
    "        # If there are more than n values/data, stop. \n",
    "        if total_scraped > n:\n",
    "            break\n",
    "        \n",
    "        # Generate a random sleep duration to seem like a human user\n",
    "        sleep_duration = random.randint(1,9)\n",
    "        #print(f'sleep_duration: {sleep_duration}')\n",
    "        time.sleep(sleep_duration)\n",
    "            \n",
    "    \n",
    "    # Creating a list of features of interest that we will be using\n",
    "    features_of_interest = ['subreddit', 'title', 'selftext']\n",
    "    \n",
    "    # Combine all iterations into 1 dataframe\n",
    "    final_df = pd.concat(posts, sort=False)\n",
    "    # And remove dataframe to limit to features of interest\n",
    "    final_df = final_df[features_of_interest]\n",
    "    # Dropping any duplicates\n",
    "    final_df.drop_duplicates(inplace=True)\n",
    "    # Display final shape to double check the columns created\n",
    "    #print(f'final_df.shape: {final_df.shape}')\n",
    "    return final_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbf7ad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b869312bb7b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmissions_perfumes_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Perfumes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msubmissions_makeup_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Makeup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Retrieved {len(submissions_perfumes)} submissions on \\'Perfumes\\' from Pushshift'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Retrieved {len(submissions_makeup)} submissions on \\'Makeup\\' from Pushshift'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-79bd7d1b489c>\u001b[0m in \u001b[0;36mscrap\u001b[0;34m(subreddit, n, days)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0msleep_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m#print(f'sleep_duration: {sleep_duration}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_duration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "submissions_perfumes_df = scrap('Perfumes', 3000)\n",
    "submissions_makeup_df = scrap('Makeup', 3000)\n",
    "\n",
    "print(f'Retrieved {len(submissions_perfumes)} submissions on \\'Perfumes\\' from Pushshift')\n",
    "print((f'Retrieved {len(submissions_makeup)} submissions on \\'Makeup\\' from Pushshift'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d84f64b",
   "metadata": {},
   "source": [
    "We have managed to extract more than 3,000 non duplicate posts for both subreddits. We will export the files to csv and proceed with the cleaning and analysis of the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7194b69",
   "metadata": {},
   "source": [
    "**5. Exporting of data to csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100799f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_perfumes_df.to_csv('../datasets/perfumes_df.csv')\n",
    "submissions_makeup_df.to_csv('../datasets/makeup_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69edf3d2",
   "metadata": {},
   "source": [
    "We will continue the rest of the analysis in a separate workbook. Please refer to **\"2. Analysis of Datasets\"** for the analysis and recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
